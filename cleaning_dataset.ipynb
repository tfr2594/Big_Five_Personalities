{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_questionnaire = \"../data-final.csv\"\n",
    "\n",
    "#when reading the file you have to add sep='\\t', if you don't it's a pain to fix it later down the line\n",
    "\n",
    "base_questionnaire_df = pd.read_csv(base_questionnaire, sep='\\t')\n",
    "\n",
    "#check if it works, can also use .head()\n",
    "\n",
    "base_questionnaire_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the original DataFrame to get the rows with missing data. The reasoning behind it, is that some people will not finish the test and it will be incomplete slightly alterting the results\n",
    "\n",
    "missing_data_df = base_questionnaire_df[base_questionnaire_df.isnull().any(axis=1)]\n",
    "\n",
    "#missing_data_df is the data with missing data that we might want to go back and analyze in its own analysis. Roughly 3,200 rows dropped\n",
    "\n",
    "base_questionnaire_df = base_questionnaire_df.dropna()\n",
    "\n",
    "#check if it works, look at the number of rows\n",
    "\n",
    "base_questionnaire_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In the codebook.txt the data collection noted that \"For max cleanliness, only use records where this value is 1. High values can be because of shared networks or multiple submissions\"\n",
    "#Although it will cut down half the dataset, it will make the analysis more concrete, so there isn't a person adding more than one submission. \n",
    "\n",
    "ipc_value_count = base_questionnaire_df['IPC'].value_counts()[:20].sort_values(ascending=False)\n",
    "\n",
    "#check to see the amount of IP submissions from 1 and onward\n",
    "ipc_value_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ipc_high_count is any IPC value 2 or higher\n",
    "\n",
    "ipc_high_count = base_questionnaire_df[base_questionnaire_df['IPC'] != 1]\n",
    "\n",
    "#base_questionnaire_df now only has data where IPC equals 1\n",
    "\n",
    "base_questionnaire_df = base_questionnaire_df[base_questionnaire_df['IPC'] == 1]\n",
    "\n",
    "#check the number of rows\n",
    "\n",
    "base_questionnaire_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop any duplicate rows, if there was any glitches in inputting the data.\n",
    "\n",
    "base_questionnaire_df = base_questionnaire_df.drop_duplicates()\n",
    "\n",
    "#Additionally, I like to be able to view all the column titles, to see if I can create another subset of data\n",
    "column_list = base_questionnaire_df.columns.values.tolist()\n",
    "column_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I also wanted to see if the analysis for the answers alone. I didn't want extra information clouding/altering the analysis, but I will hopefully study a bit more for country, and time it took overall and for specific questions later.\n",
    "answers_only_df = base_questionnaire_df.iloc[:, :50].copy()\n",
    "\n",
    "#check if the correct columns are showing up\n",
    "\n",
    "answers_only_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lines 7 - ___ are to creat test and training datasets... maybe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#At this moment there is not any more cleaning of data that I can think of. The only thing to keep in mind is that country, lat_appx_lots_of_err, long_appx_lots_of_err  were found by techinical information and wasn't given by the people taking the test\n",
    "#Lastly, export the smaller data set into your folder. I like doing a csv file and Excel file. Be sure to save the full and smaller clean data sets.\n",
    "\n",
    "base_questionnaire_df.to_csv(\"clean_personality_dataset.csv\", encoding=\"utf-8\", index=True)\n",
    "\n",
    "answers_only_df.to_csv(\"answer_only_dataset.csv\", encoding=\"utf-8\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lastly, export the smaller data set into your folder. I like doing a csv file and Excel file.\n",
    "writer = pd.ExcelWriter('clean_personality_dataset.xlsx', engine='xlsxwriter')\n",
    "\n",
    "#enable ZIP64 extension, the file was too big, so the file will need to be ziiped\n",
    "writer.book.use_zip64()\n",
    "base_questionnaire_df.to_excel(writer, sheet_name='Personality_Answers_Clean', index=True)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lines  - ___ are to creat test and training datasets... maybe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "b2c2cd3e1537c2b0c859cecf32b8531afe3fb7d5df0425305979daae460b9f95"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
